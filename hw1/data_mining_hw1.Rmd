---
title: "Data Mining HW1"
author: "Salem Marrero"
date: "1/2/2018"
output: html_document
---

##Problem 1

```{r}
#setwd("~/GitHub/data-mining/hw1/")
load("hw1prob1.Rdata")
```

####A

First normalize the documents by their total word count, and then apply IDF weighting
to the words. Save this matrix as dtm1. Now reverse the order: first apply IDF weighting
to the words, then normalize the documents by their total word count, and save this matrix
as dtm2. Are dtm1 and dtm2 different? Explain why you think they should or shouldn’t be
different.


```{r}
dtm1 = dtm/rowSums(dtm)
idf1 = log(nrow(dtm1)/colSums(dtm1 != 0))
for (doc in 1:nrow(dtm1)){
  dtm1[doc,] = dtm1[doc,] * idf1
}

dtm2 = dtm
idf2 = log(nrow(dtm2)/colSums(dtm2 != 0))
for (doc in 1:nrow(dtm2)){
  dtm2[doc,] = dtm2[doc,] * idf2
}
dtm2 = dtm2/rowSums(dtm2)
```


```{r}
rowSums(dtm1)
rowSums(dtm2)
```

dtm1 had IDF applied after row normailization, so its rows no longer sum to 1.

####B

We’re going to forgo IDF weighting with such a small collection (8 documents). Normalize
the documents by their total word count, and call this matrix dtm3. According to
this document-term matrix, which document is closest (measured in Euclidean distance) to
the document named “tmnt mike”?

```{r}
dtm3 = dtm/rowSums(dtm)
###currently incomplete 
euclidean_distances = list()
for (i in 1:nrow(dtm3)){
  euclidean_distances[row.names(dtm3)[i]] = sqrt(sum((dtm3["tmnt mike",] - dtm3[i,])**2))
}
euclidean_distances
```

tmnt raph has the closest Euclidean Distance to tmnt mike. 

####C

Sticking with the normalized matrix dtm3, compute the distance between each pair of
documents using the function dist. Now run hierarchical agglomerative clustering, both
with single linkage and complete linkage, using the function hclust. Plot the resulting
dendograms for both linkages. If you had to split the documents into 2 groups, which
linkage do you think gives a more reasonable clustering?


```{r}
d = dist(dtm3)
tree.sing = hclust(d,method="single")
tree.comp = hclust(d,method="complete")
par(mfrow=c(1,2))
plot(tree.sing,labels=F,hang=-1e-10)
plot(tree.comp,labels=F,hang=-1e-10)
```


complete gives a much better dendrogram because the first split partitions the data into two sets of 4. 

